{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import sys\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./assets/Corona_NLP_train_clean.csv\", encoding=\"latin1\")\n",
    "test = pd.read_csv(\"./assets/Corona_NLP_test_clean.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classes_def(x):\n",
    "    if x ==  \"Extremely Positive\":\n",
    "        return \"2\"\n",
    "    elif x == \"Extremely Negative\":\n",
    "        return \"0\"\n",
    "    elif x == \"Negative\":\n",
    "        return \"0\"\n",
    "    elif x ==  \"Positive\":\n",
    "        return \"2\"\n",
    "    else:\n",
    "        return \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train.OriginalTweet\n",
    "train[\"text\"] = train[\"text\"].astype(str)\n",
    "\n",
    "test['text'] = test.OriginalTweet\n",
    "test[\"text\"] = test[\"text\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['label']=train['Sentiment'].apply(lambda x:classes_def(x))\n",
    "test['label']=test['Sentiment'].apply(lambda x:classes_def(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libs.preprocessing\n",
    "from libs.embedding import vectorize_with_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_raw = train[\"text\"].tolist()\n",
    "y_train = train[\"label\"].tolist()\n",
    "X_test_raw = test[\"text\"].tolist()\n",
    "y_test = test[\"label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorize_with_glove(X_train_raw)\n",
    "X_test = vectorize_with_glove(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid_model(classifier, param_grid, n_splits, test_size):\n",
    "    cv = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size)\n",
    "    grid_model = GridSearchCV(classifier, param_grid=param_grid, cv=cv, n_jobs=-1, verbose=1, scoring='accuracy')\n",
    "    return grid_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(grid_model, predicted_test, test_label, predicted_train, train_label, file_name, \n",
    "           decision_function, clf_name=\"Classifier\"):\n",
    "    print(\"Results for \", clf_name, \": \")\n",
    "    print()\n",
    "    print(\"Best parameters are %s\" % (grid_model.best_params_))\n",
    "    acc_train = accuracy_score(train_label, predicted_train)\n",
    "    acc_test = accuracy_score(test_label, predicted_test)\n",
    "    print(\"Train Accuracy:  %0.3f\" % (acc_train))\n",
    "    print(\"Validation Accuracy:   %0.3f\" % (grid_model.best_score_))\n",
    "    print(\"Test Accuracy   %0.3f\" % (acc_test ))\n",
    "\n",
    "\n",
    "    print(\"Mean training time: %f\" % (np.mean(grid_model.cv_results_['mean_fit_time'], axis=0)) )\n",
    "    print(\"Mean test time: %f\" % (np.mean(grid_model.cv_results_['mean_score_time'], axis=0)) )\n",
    "    \n",
    "    # confusion matrix\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    print ( confusion_matrix(test_label, predicted_test) )\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print ( classification_report(test_label, predicted_test) )\n",
    "    \n",
    "    \n",
    "    f = open(file_name+'.txt','w')\n",
    "    f.write(\"Best parameters are %s\\n\"% (grid_model.best_params_))\n",
    "    f.write(\"Train Accuracy: %0.3f\\n\" % (acc_train))\n",
    "    f.write(\"Validation Accuracy: %0.3f\\n\" % (grid_model.best_score_))\n",
    "    f.write(\"Test Accuracy: %0.3f\\n\" % (acc_test ))\n",
    "    f.write(\"\\nConfusion Matrix:\")\n",
    "    f.write(str(confusion_matrix(test_label, predicted_test)) + \"\\n\")\n",
    "    f.write(\"\\nClassification Report:\")\n",
    "    f.write(str(classification_report(test_label, predicted_test)) + \"\\n\\n\")\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   11.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   11.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for  RandomForest : \n",
      "\n",
      "Best parameters are {'max_depth': 4, 'max_features': 'sqrt', 'n_estimators': 100}\n",
      "Train Accuracy:  0.532\n",
      "Validation Accuracy:   0.522\n",
      "Test Accuracy   0.502\n",
      "Mean training time: 7.235193\n",
      "Mean test time: 0.037036\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 516    8 1109]\n",
      " [ 136   26  457]\n",
      " [ 178    5 1363]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.32      0.42      1633\n",
      "           1       0.67      0.04      0.08       619\n",
      "           2       0.47      0.88      0.61      1546\n",
      "\n",
      "    accuracy                           0.50      3798\n",
      "   macro avg       0.58      0.41      0.37      3798\n",
      "weighted avg       0.57      0.50      0.44      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier() # classifier\n",
    "\n",
    "# classifier parameters\n",
    "max_depth = [4] #[2, 4, 8, 10, 20]\n",
    "n_estimators = [100] # [100, 500, 1000]\n",
    "max_features = [2 ,'sqrt']\n",
    "# min_samples_split = [None, 10, 50]\n",
    "\n",
    "# cross validation\n",
    "folds = 2\n",
    "test_size = 0.1\n",
    "\n",
    "embedding = \"glove\"\n",
    "# parameters to grid search\n",
    "param_grid = dict(max_depth=max_depth, n_estimators=n_estimators, max_features=max_features)\n",
    "\n",
    "rfc_model = create_grid_model(rfc, param_grid, folds, test_size)\n",
    "rfc_model.fit(X_train, y_train)\n",
    "\n",
    "predicted_test = rfc_model.predict(X_test)\n",
    "predicted_train = rfc_model.predict(X_train)\n",
    "decision_function = rfc_model.predict_proba(X_test)\n",
    "\n",
    "evaluation(rfc_model, predicted_test, y_test, predicted_train, y_train, \n",
    "       \"./results/rfc_\"+embedding, decision_function, clf_name=\"RandomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
